Graphics processing units (GPUs) have become more general purpose and are increasingly 
used for a wider range of applications. As an accelerator device, however, a conventional
discrete GPU only allows access to its own device memory, so programmers need to design
their applications carefully to fit in the device memory. This makes it very challenging and
costly to run large-scale applications with hundreds of GBS of memory footprint, such as graph
Computing workloads, because it requires careful data and algorithm partitioning in addition to 
purchasing more GPUs just for memory capacity. To address this issue, recent GPUs support Unified 
Virtual Memory(UVM). UVM provides a coherent view of a single virtual address space between CPUs 
and GPUs with automatic data migration via demand paging. This allows GPUs to access a page that 
resides in the CPU memory as if it were in the GPU memory, thereby allowing GPU applications to 
run without worrying about the device memory capacity limit. As such, UVM frees programmers from 
tuning an application for each individual GPU and allows the application to run on a variety 
of GPUs with different physical memory sizes without any source code changes. This is good for 
programmability and portability.

While the feature sounds promising, in reality, the benefit comes with a non-negligible performance 
cost. Virtual memory support requires address translation for every memory request, and its performance 
impact is more substantial than in CPUs because GPUs can issue a significantly larger number of memory 
requests in a short period of time. In addition, paging in and out of GPU memory requires costly
communications between CPU and GPU over an interconnect such as PCIe and an interrupt handler 
invocation. Prior work reports that page fault handling latency ranges from 20µs to 50µs.
Unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden 
even with ample thread-level parallelism (TLP) in GPUs.

Recently, Debashis proposed a simulation framework, called UVM Smart, to provide
both functional and timing simulation support for UVM. To mitigate the costly page-faults handling, 
his work explores various hardware prefetches in the context of FPU's unified memory management. 
His result shows prefetching larger chunks of memory improves PCI-e utilization and reduces transfer 
latency. Further, prefetched pages reduce the number of page-faults and in turn the overhead to resolve them.