We use GPGPU-Sim simulator as a library to provide CTA scheduling and GPU core
functionality. GPU Scheduler and SM Group components invokes on GPGPU-Sim library
(libcudart.so) with internal API calls.

To support SST-GPU to run on one thread, on multiple threads or on multiple
processes on one node or multiple nodes, we refactor GPGPU-Sim simulator to avoid
static and global variables. Instead, GPU components construct a GPU context data
structure before using GPGPU-Sim library so that each component can keep an
individual context. Moreover, we design an option inside the context data structure
to manage the library in scheduler mode or SM mode. GPGPU-Sim library works as
a CTA scheduler to issue CTAs in scheduler mode and works as a group of GPU cores in SM mode.

GPGPU-Sim simulator separates functional model with performance model, so the
instruction operations are simulated on the issue pipeline stage. However, the load store
unit (LSU) sends out memory requests to GPU memory hierarchy on the execution stage.
This design breaks if SM groups need to access the GPU memory components.
Therefore, we replay the memory instruction operations after the data returned
to LSU in SM mode.
