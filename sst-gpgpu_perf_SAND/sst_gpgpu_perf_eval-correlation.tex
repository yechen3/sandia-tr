A validation sweep was run using two kernels and a mini-app. The three
applications were run using an SST model that approximates a Nvidia V100
attached to a CPU. The simulation parameters are shown in Table
\ref{tab:v100_params}. The overall kernel runtime was compared with the results
of running the three applications through nvprof on Sandia's Waterman testbed,
which is comprised of IBM Power9 CPUs and Nvidia Volta GPUs. Table \ref{tab:correlation}
shows the total number cycles that each application took on the SST-GPU model
and on the native V100. Note that this is only cycles where a kernel was running
and does not include host execution time. There are challenges isolating the
cause of the performance gaps. This is one of the largest, if not the largest,
node simulation that has been run with 139 unique components and 906 links (the
statistics output contains nearly 20k unique entries). The complex model
interactions and scale make it difficult to pinpoint where models are lacking in
detail or are incorrect. Turning on debug for even a small run can produce
multi-terabyte output files. That being said, the authors do have some intuition
into why there are gaps and how to close them.


    \begin{table}[!htbp]
      \centering
      \setlength{\belowcaptionskip}{6pt plus 1pt minus 1pt}
      \captionsetup{width=.75\textwidth}
      \caption{CPU/V100 Model Parameters}
      \subtable[CPU]{%
         \begin{tabular}{|l|c|}
            \hline
            Clock                   & 2660MHz  \\ \hline
            DDR Clock               & 2666     \\ \hline
            DDR Capactiy            & 16384MiB \\ \hline
            Mesh Frequency          & 800MHz   \\ \hline
            Mesh Input Ports        & 1        \\ \hline
            Mesh Output Ports       & 1        \\ \hline
            Data Link Latency       & 23840ps  \\ \hline
            Command Link Latency    & 23840ps  \\ \hline
         \end{tabular}
      }
      \hspace{1cm}
      \subtable[GPU]{%
         \begin{tabular}{|l|c|}
            \hline
            Clock                 & 1312MHz          \\ \hline
            SMs                   & 84               \\ \hline
            L2 Slices             & 32               \\ \hline
            L2 Capactiy           & 192KiB per slice \\ \hline
            HBM Capacity          & 16384MiB         \\ \hline
            HBM Stacks            & 4                \\ \hline
            Crossbar Frequency    & 1200MHz          \\ \hline
            Crossbar Input Ports  & 2                \\ \hline
            Crossbar Output Ports & 1                \\ \hline
         \end{tabular}
      }
      \label{tab:v100_params}
   \end{table}

   \begin{table}[!htbp]
      \centering
      \setlength{\belowcaptionskip}{6pt plus 1pt minus 1pt}
      \captionsetup{width=.75\textwidth}
      \caption{SST-GPGPU Correlation}
      \begin{tabular}{l|r|r|r|}
         \cline{2-4}
                                                & \multicolumn{1}{c|}{\textbf{P9/V100}} & \multicolumn{1}{c|}{\textbf{SST-GPGPU}} & \multicolumn{1}{c|}{\textbf{Error}} \\ \hline
         \multicolumn{1}{|l|}{\textbf{vectorAdd}} & 5271                                  & 5751                                    & 9.09                              \\ \hline
         \multicolumn{1}{|l|}{\textbf{lud}}       & 494519                                & 605685                                  & 22.48                             \\ \hline
         \multicolumn{1}{|l|}{\textbf{lulesh}}    & 12454750                              & 11896477                                & 4.48                              \\ \hline
         \end{tabular}
      \label{tab:correlation}
   \end{table}


\subsection{Vector Addition}
\label{sec:vecadd}
The vectorAdd application is from the Cuda SDK with error checking removed. It
implements element by element vector addition using an array with 163840
elements.

vectorAdd contains a single kernel with a single invocation that, essentially,
streams through memory performing integer operations. It was expected that this
would have a higher correlation, but the fact that there are so many memory
dependencies and memory operations make the results highly dependent on the
model for the backing store. A number of models were tried and flaws were found
in all of them. With the exception of Cramsim, all of the models are derived
from simple DRAM models and are unable to accurately replicate the behavior of
HBM. It is believed that there is an issue in the memory controller that Cramsim
uses and that when this is solved, it will serve as a good model for HBM2. However,
the timingDRAM model clearly provides enough detail for kernels that are not
bottle-necked by memory bandwidth.


\subsection{LU Decomposition}
\label{sec:lud}
The lud application is from the Rodinia benchmark
suite~\cite{rodinia_5306797}\cite{rodinia_5650274} and implements the LU
decomposition algorithm to solve a set of linear equations using a 256x256
element matrix.

The lud application from Rodinia contains 3 kernels with 46 total kernel
launches. lud has the worst correlation. The \texttt{perimeter} and
\texttt{diagonal} kernels occupy the majority of the compute time --
\texttt{diagonal} has 16 invocations and consumes 63\% of the time;
\texttt{perimeter} has 15 invocations and consumes 22\% of the time;
\texttt{internal} has 15 invocations and consumes 14\% of the time.
\texttt{perimeter} and \texttt{diagonal} spend 50\% and 80\% of their time
inactive, respectively, due to the number of divergences. Given that LULESH has
a much greater diversity of instructions, including FP64, and the previously
reported issues determining control flow, it's unlikely that the problem lies in
the ALU models and more likely that the issues stem from how the GPU model
handles divergences or complex issues exposed by the differences in using PTX
verses SASS.


\subsection{LULESH}
\label{sec:lulesh}
LULESH is one of the most widely used mini-applications developed by the US
Department of Energy. The code was originally developed by Lawrence Livermore
National Laboratory to represent challenging hydrodynamics algorithms that are
performed over unstructured meshes~\cite{lulesh:spec}\cite{lulesh:changes}.
Such algorithms are common in many high-performance computing centers and are
particularly prevalent within the NNSA laboratories. In the original LULESH
specification, the authors state that such algorithms routinely count in the top
ten application codes in terms of CPU hours utilized~\cite{lulesh:spec}.

The unstructured nature of LULESH presents challenges for the design of memory
subsystems, not least because operands are gathered from a fairly limited locale
but are done so sparsely. This makes efficient streaming and vectorization of
the data operations difficult and places additional pressure on the memory
subsystem (typically the L2 caches) to provide operands quickly.

For this experiment, the problem size was set to 22 with 50 iterations, leading
to an application that contains 26 kernels with 1400 total invocations. The top
three kernels, in terms of execution time, provided a good mix of operations,
shown in Table \ref{tab:lulesh}. The diversity of operations in lulesh, compared
to the other too applications, obfuscates the areas where the simulation is
lacking, leading to higher correlation with the V100 target platform.

It's clear that a more detailed study is needed to isolate the weaknesses in the models.


   \begin{table}[!htbp]
      \centering
      \setlength{\belowcaptionskip}{6pt plus 1pt minus 1pt}
      \captionsetup{width=.75\textwidth}
      \caption[LULESH Instruction Count Percentages]{LULESH Instruction Count Percentages (nvprof)}
      \begin{tabular}{l|c|c|c|c|c|c|c|}
         \cline{2-8}
                                                                     & \textbf{FP32} & \textbf{FP64} & \textbf{INT} & \textbf{CTRL} & \textbf{L/S} & \textbf{MISC} & \textbf{INACTIVE} \\ \hline
         \multicolumn{1}{|l|}{\textbf{CalcFBHourglassForceForElems}} & 1             & 10            & 11           & 10            & 12           & 31            & 23                \\ \hline
         \multicolumn{1}{|l|}{\textbf{CalcPressureForElems}}         & 5             & 17            & 27           & 2             & 19           & 16            & 15                \\ \hline
         \multicolumn{1}{|l|}{\textbf{CalcHourglassControlForElems}} & 0             & 25            & 21           & 3             & 38           & 9             & 1                 \\ \hline
      \end{tabular}
      \label{tab:lulesh}
   \end{table}


